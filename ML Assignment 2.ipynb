{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f85ed-7d16-4b8d-8a2a-ad707295a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "+-\n",
    "'''\n",
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how \n",
    "can they be mitigated?\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e88f00a-01a7-443c-93ae-dbb14147ae79",
   "metadata": {},
   "source": [
    "#Ans 1:\n",
    "### Overfitting: \n",
    "In machine learning, when the accuracy of the model is high for training data set but the moment we use the test dataset the accuracy decreases, this situation is called overfitting. Here it has a low bias and high variance.\n",
    "\n",
    "Consequences:\n",
    "1. It is useless for real world application.\n",
    "2. The model prediction is highly sensitive to any small change in the data, making it unreliable.\n",
    "\n",
    "Mitigation:\n",
    "1. Use a validation dataset to increase the accuracy of the model.\n",
    "2. Increase the volume of training data to diversify it.\n",
    "\n",
    "### Underfitting:\n",
    "In machine learning, when the accuracy of the model is low for training data set and also when we use the test dataset the accuracy decreases, this situation is called underfitting. Here it has a high bias and high variance.\n",
    "\n",
    "Consequences:\n",
    "1. The model prediction is biased.\n",
    "2. The prediction by model is inaccurate.\n",
    "\n",
    "Mitigation:\n",
    "1. Provide model with more information to improve the accuracy.\n",
    "2. Create new feature that better represent the data and capture relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf073d1b-0e50-4704-92ad-3ac1e2567bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q2: How can we reduce overfitting? Explain in brief.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83ce6be3-048e-4b9c-8b52-86b161a2e1e8",
   "metadata": {},
   "source": [
    "#Ans 2:\n",
    "We can reduce overfitting by the following ways:\n",
    "\n",
    "1. Use a validation dataset to increase the accuracy of the model.\n",
    "2. Increase the volume of training data to diversify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7675f3b2-38f5-4c8e-8cb9-f5c7c58d39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "445e598a-75ae-46df-8d81-5d32d8c584a9",
   "metadata": {},
   "source": [
    "#Ans 3:\n",
    "In machine learning, when the accuracy of the model is low for training data set and also when we use the test dataset the accuracy decreases, this situation is called underfitting. Here it has a high bias and high variance.\n",
    "\n",
    "The scenarios are:\n",
    "1.If the training dataset is too small, the model may not have enough information to learn the underlying patterns effectively.\n",
    "2.Including features that are not relevant to the target variable can introduce noise and make it harder for the model to learn the important relationships.\n",
    "3.Necessary transformations or scaling might be missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c8e70-9e5c-4b1e-9c8e-b30d02421205",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and \n",
    "variance, and how do they affect model performance?\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d1bc943-06b4-48ea-9223-a757bf3a732c",
   "metadata": {},
   "source": [
    "#Ans 4:\n",
    "Bias-Variance trade-off talks about the balance between two main sources of error in our predictive model i.e. bias and variance.\n",
    "\n",
    "Bias is basically the error of the training dataset. And variance is the error of the testing dataset.\n",
    "\n",
    "Bias and variance have an inverse relationship. Reducing one often increases the other. For example, a more complex model with higher capacity (e.g., a neural network) might reduce bias by capturing more complex patterns, but it can also increase variance by being more sensitive to training data fluctuations.\n",
    "\n",
    "The impact are:\n",
    "1.The goal of machine learning is to minimize both bias and variance to achieve optimal performance. A model with low bias and low variance will generalize well to unseen data and provide accurate predictions.\n",
    "\n",
    "2.High bias leads to underfitting, resulting in inaccurate predictions for both training and test data.\n",
    "\n",
    "3.High variance leads to overfitting, resulting in accurate predictions on the training data but poor performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433e24e-bd19-4ba5-a764-f21b1270e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. \n",
    "How can you determine whether your model is overfitting or underfitting?\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d43d2e3-b482-4b01-a828-30b54d6e6322",
   "metadata": {},
   "source": [
    "#Ans 5:\n",
    "The various ways to detect overfitting and underfitting are:\n",
    "\n",
    "1.Evaluating performance on Training and validation dataset-> If the accuracy in training dataset is high but in validation dataset it's low then the model is overfitted. Also if the accuracy is low in both training and validation dataset then the model is underfitted.\n",
    "\n",
    "2.Plot training and validation accuracy against the number of training epochs.\n",
    "\n",
    "3.Examining Confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512df40d-f3d4-41b9-bb18-9d7e28d63007",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias \n",
    "and high variance models, and how do they differ in terms of their performance?\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fee9da9-a303-4015-92c0-29012596495f",
   "metadata": {},
   "source": [
    "#Ans 6:\n",
    "Bias: It is basically the error of our training dataset. When the accuracy of our training dataset is low it leads to a high bias which is called underfitting.\n",
    "\n",
    "Variance: It is basically the error of our test dataset. When the accuracy of our test training dataset is low it leads to a high variance which is called overfitting.\n",
    "\n",
    "Examples of high bias:\n",
    "1.Linear Regression for a highly non-linear problem.\n",
    "2.A decision tree with only one level\n",
    "\n",
    "Examples of high variance:\n",
    "1.Neural network with too many hidden layers and insufficient training data.\n",
    "2.K-Nearest Neighbors classifier with k=1 (very sensitive to noisy data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547b0ab-569a-4df4-93cd-0c34c0630e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe \n",
    "some common regularization techniques and how they work.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5b4e98f-1f1a-4171-8d32-824475ac6c97",
   "metadata": {},
   "source": [
    "#Ans 7:\n",
    "Regularization is a set of techniques used in machine learning to address the problem of overfitting, where a model memorizes the training data too well and fails to generalize to unseen data\n",
    "\n",
    "1. L1 and L2 Regularization:\n",
    "\n",
    "-->Add a penalty term to the cost function based on the sum of the absolute values (L1) or squares (L2) of the model's coefficients (weights).\n",
    "-->L1 regularization: Encourages sparsity, forcing some coefficients to become zero, effectively removing their influence on the model. This can be good for feature selection.\n",
    "-->L2 regularization: Shrinks all coefficients towards zero, reducing their overall influence and preventing the model from overfitting to specific data points.\n",
    "\n",
    "2. Weight Decay:\n",
    "\n",
    "-->Similar to L2 regularization, but the penalty term is directly applied to the weights during each training iteration.\n",
    "-->Helps control the learning rate of individual weights, preventing them from growing too large and contributing to overfitting.\n",
    "\n",
    "3. Early Stopping:\n",
    "\n",
    "-->Monitor the model's performance on a validation set during training.\n",
    "-->Stop training before the model starts to overfit based on the validation set performance.\n",
    "-->This method effectively prevents the model from learning irrelevant details from the later stages of training data.\n",
    "\n",
    "4. Data Augmentation:\n",
    "\n",
    "-->Artificially increase the training data by applying transformations (rotations, flips, etc.) to existing data points.\n",
    "-->This exposes the model to a wider range of patterns and reduces its overfitting to the specific training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39315665-2a34-43ea-8443-de15080acea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
